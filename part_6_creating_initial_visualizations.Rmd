---
title: "Visualizing Arlington Bikometers"
subtitle: "Part 5: Creating the Initial Visualizations"
output:
  html_document: 
    toc: yes
    toc_depth: 2
    toc_float: yes
    highlight: zenburn
    code_download: true
    df_print: paged
    includes:
      in_header: header.html
---
\ 
\ 

# Goal

Create a post with similar parameters to the [Minnesota DOT's](https://storymaps.arcgis.com/stories/70a3a57003b541ed8e911cd11103dc6a) *Covid-19 & Historic Total Traffic Count* graph.

# Connect to MySQL Database

```{r echo = FALSE, message = FALSE, warning= FALSE, include=TRUE}
library(ggplot2)
library(dplyr)
library(odbc)
library(reshape2)
library(tidyr)

con <- dbConnect(odbc::odbc(), dsn = "Bike_MySQL")
```

# Load the table into R

```{r}
counts_daily <- tbl(con, 'counts_daily')
head(counts_daily)
```

# Combining 'In' and 'Out' Values

Below is the code we created in In [Part 5](https://nathansprojects.com/part_5_picking_bikeometers_to_graph.html), to combine the 'I' and 'O' values.

```{r warning=FALSE}
counts_daily_total <- counts_daily %>% 
  group_by(date, bikeometer_id) %>% 
  summarize(count = sum(count), is_weekend = is_weekend, 
            month = month, day = day, year = year, month_day = month_day) %>% 
  collect()
counts_daily_total
```

```{r}
class(counts_daily_total$month_day)
```

I'm going to manipulate the counts_daily_total 'tibble' using the *mutate* function.

I realized that I should have used a hyphen instead of an underscore for 'month-day' rows so first I need to do some character replacement. I'll use the *mutate()* function to make changes to columns and pass in the *gsub()* function to replace the underscore with a hyphen.   

Then, we will add an arbitrary year to the end of month_day to make it easier to convert to a *date* class and subsequently graph.

```{r}
counts_daily_total <- counts_daily_total %>% mutate(month_day = gsub("[_]", "-", month_day))

counts_daily_total <- counts_daily_total %>% mutate(month_day=paste(month_day, '-2020', sep = ''))
counts_daily_total

```
Now, let's convert month_day to a *date* class.

```{r}
class(counts_daily_total$month_day[1])

counts_daily_total$month_day <- strptime(counts_daily_total$month_day, '%m-%d-%Y')
class(counts_daily_total$month_day[1])

counts_daily_total$month_day <- as.Date(counts_daily_total$month_day)
class(counts_daily_total$month_day[1])
```
```{r}
counts_daily_total
```

### Note to self
Originally, class(counts_daily_total$year,month,day == 'S3 value Bigint' which doesn't play nicely with dplyr. Be sure to convert number columns into *int* before applying filters. Also, for when mutating a column, don't assign the mutate like this:

```{r eval=FALSE}
# Don't use this
counts_daily_total$month_day <- counts_daily_total %>% mutate(month_day = gsub("[_]", "-", month_day))

# Use this!
counts_daily_total <- counts_daily_total %>% mutate(month_day = gsub("[_]", "-", month_day))

```

# Filtering for Date Range and Bikeometer ID

In a previous post, I chose a time-frame and which Bikeometers were best to plot. 

```{r}
counts_daily_filtered_2017_to_2019 <- counts_daily_total %>% 
  filter(date >= '2017-03-12' & date <= '2017-05-15' & bikeometer_id %in% c(14,15,16,18,22,31,39)| 
         date >= '2018-03-12' & date <= '2018-05-15' & bikeometer_id %in% c(14,15,16,18,22,31,39)|
         date >= '2019-03-12' & date <= '2019-05-15' & bikeometer_id %in% c(14,15,16,18,22,31,39))
counts_daily_filtered_2017_to_2019
```
```{r}
counts_daily_filtered_2020 <- counts_daily_total %>% 
  filter(date >= '2020-03-12' & date <= '2020-05-15' & bikeometer_id %in% c(14,15,16,18,22,31,39))

counts_daily_filtered_2020
```

# Group the 2020 Bikeometer counts

Now we can can group the counts by taking their average. I'll rename the column '2020_counts' to make it easier to merge the table later.

```{r}
counts_daily_filtered_2020_grouped <- counts_daily_filtered_2020 %>% group_by(month_day) %>% summarise('2020_counts' = mean(count))

counts_daily_filtered_2020_grouped
```

# Creating the 2017-2019 Daily Average column

Now that we have two tables, one with 2017-2019 data and one with 2020 data, let's add a column to the counts_daily_filtered_2017_to_2019 table that shows the average across all Bikeometers.

```{r}
counts_daily_filtered_2017_to_2019_grouped <- counts_daily_filtered_2017_to_2019 %>% group_by(month_day) %>% summarise('2017-2019_avg_counts' = mean(count))

counts_daily_filtered_2017_to_2019_grouped
```
We should check to see if the average is being taken correctly. We can manually check the first 'month_day' below.

```{r}
counts_daily_filtered_2017_to_2019 %>% select(month_day, count, date, bikeometer_id) %>%  filter(month_day =='2020-03-12') 
```

```{r}
(16 + 140 + 34 + 51 + 250 + 131 + 108 + 28 + 228 + 54 + 48 + 408)/12
```

This matches the 'avg_counts' value for the '2020-03-12' date! 

```{r}
counts_daily_filtered_2020_grouped
```
```{r}
counts_daily_filtered_2017_to_2019_grouped
```

Let's now join the 'counts_daily_filtered_2017_to_2019_grouped' and the 'counts_daily_filtered_2020_grouped' tables so we can graph them.


```{r}
df <- merge(x = counts_daily_filtered_2017_to_2019_grouped, y = counts_daily_filtered_2020_grouped, by = 'month_day')
df
```

In order for this table to be easily read by R, we need to covert it from a wide table to a long table. [This]('https://mgimond.github.io/ES218/Week03b.html') is the resource I used to understand how to do the conversion.

```{r}
df.long <- pivot_longer(df, cols=2:3, names_to='year', values_to='counts')
df.long
```

```{r}
line <- df.long %>% ggplot(aes(month_day, counts, group = year, color = year)) +
  geom_area(aes(fill = year, group = year), alpha = 0.5, position = 'identity')
line
```
If you don't include 'position = 'identity' then the counts will stack on top of each other instead of overlapping like above.

You can see the default arguments for a plot by using the *args()* function.

```{r}
args(geom_area)
```

```{r}
library(ggthemes)
line <- line + theme_calc() + labs(x = 'Date', title = 'COVID-19 & Historic Total Bike Count')
line
```

```{r}
library(plotly)
library(ggplot2)
ggplotly(line)
```

It looks like there definitely was an increase for some days during the 2020 COVID Pandemic but the data isn't as significant as the MnDOT plot. 

Let's investigate this a little.

# Investigating Bikeometer Locations

I'll map the locations of the Bikeometers to see where exactly they are in Arlington. To accomplish this, I found [this video](https://www.youtube.com/watch?v=2k8O-Y_uiRU&t=3s) by Professor Lisa Lendway very helpful along with [her website](https://mapping-in-r.netlify.app/).

![](images/Arlington%20Open%20Street%20Map.png "Arlington Open Street Map bbox")

On the left are the numbers we need to enter into the bbox variable below in order to get the graph from [Stamen Maps](http://maps.stamen.com/). I've chosen 'terrain' as my map type and the 'zoom' will be 13, which is in the openstreetmap url just after the text 'map='.

```{r message=FALSE}
library(ggmap)
library(ggthemes)
# Get the map information
arlington <- get_stamenmap(
    bbox = c(left = -77.2377, bottom = 38.8075, right = -76.9744, top = 38.9080), 
    maptype = "terrain",
    zoom = 12)
```

Next we will import the Bikeometer data from my database that contains the longitude and latitudes needed to plot the Bikeometers.

```{r}
sql_cmd <- "SELECT * FROM counts.bikeometer_details WHERE bikeometer_id in (14,15,16,18,22,31,39)"
# creates a lazy table
bikeometer_table <- dbGetQuery(con, sql_cmd)
bikeometer_table
```

One Bikeometer stands out as not even being in Alrington... how did that get in there? Let's plot it anyway to see how close it is to Arlington. Realistically, I frequently bike into Alexandria on the weekends but I'll probably end up excluding it none the less.

```{r}
# Convert latitude and longitude to integers
bikeometer_table$longitude <- as.numeric(bikeometer_table$longitude)
bikeometer_table$latitude <- as.numeric(bikeometer_table$latitude)

# Plot the points on the map
ggmap(arlington) + # creates the map "background"
  geom_point(data = bikeometer_table, 
             aes(x = longitude, y = latitude, color = region), 
             alpha = 0.5, 
             size = 3) +
             theme_map()
```

From my experience, the most popular bike trail in Arlington will be on the Arlington Loop. I want to get a sense if these Bikeometers are on the Arlington Loop, and if not, choose Bikeometers that are on the loop. My hypothesis is that bike trails that weren't popular before the pandemic won't see much change after the pandemic. I generally either ride the Arlington Loop or the W&OD Trails as they are the most bike-friendly. A trail that is less bike-friendly before the pandemic won't become that much more bike-friendly during a pandemic, even if there is a reduction of traffic in the city.

I'll use the GPS coordinates from my Garmin GPS watch to create a layer for my plot that shows the Arlington Loop trail. I downloaded the gpx file from the Garmin Connect website.

![](images/GPX%20file%20from%20Garmin%20Connect%20Website.jpg "Garmin Connect GPX")

I used the websites [GPXStudio](https://gpxstudio.github.io/) and [MyGeodata Converter](https://mygeodata.cloud/converter/gpx-to-csv) to remove unnecessary data points and convert from GPX to CSV respectively.

I read in the CSV which I'll use to plot the Arlington Loop.

```{r}
arlington_loop_table <- read.csv('~/Github/Arlington_Bikeometer_Visualizations/Data/arlington_loop.csv')
arlington_loop_table
```

I passed the 'arlington' map object to ggmap to set the background image to Arlington then plotted the Arlington Loop using geom_point. I used the 'longitude' column as the x-axis, 'latitude' column as the y-axis, 'alpha' is the transparency of the points (0 to 1) and 'size' is the size of each point. I used 'theme_map' without 'x' and 'y' axes.

```{r}
ggmap(arlington) + # creates the map "background"
  geom_point(data = arlington_loop_table, 
             aes(x = longitude, y = latitude), 
             alpha = 0.5, 
             size = .5) +
             theme_map()
```

Next, I'll create an Arlington Loop layer so I can overlay it on top of the Bikeometer location map. I found two options to create this layer: geom_polygon or geom_path. The biggest diffrence I see is that you can specify a fill for geom_polygon, but considering I don't want to fill the layer in, I'll use geom_path.

```{r}
ggmap(arlington) + geom_polygon(aes(x = longitude, y = latitude),
                             data = arlington_loop_table, fill = 'blue', alpha = 0.1, size = 1, color = "red")
```

```{r}
ggmap(arlington) + geom_path(data = arlington_loop_table, aes(x = longitude, y = latitude), size = 1, color = 'red') + theme_map()

```

If I wanted the path to change color according to elevation, I could use the below \<color = ele\> argument. This will generate a legend on the map. By adding the \<theme(legend.background = element_blank())\> code, you remove the background from the legend. However I won't be using this code.

```{r}
ggmap(arlington) + geom_path(data = arlington_loop_table, aes(x = longitude, y = latitude, color = ele), size = 1) + theme_map() + theme(legend.background = element_blank())
```

I'll assign the geom_path to the arlington_loop_layer variable to be plotted on top of the plot with the location of the Bikeometes.

```{r}
arlington_loop_layer <- geom_path(data = arlington_loop_table, aes(x = longitude, y = latitude), size = 1, color = 'red')
```

```{r}
ggmap(arlington) + # creates the map "background"
  geom_point(data = bikeometer_table, 
             aes(x = longitude, y = latitude, color = region), 
             alpha = 0.5, 
             size = 3) +
              arlington_loop_layer +
             theme_map()
```

It looks like 1 or 2 Bikeometers are on the trail, I'm not sure of the accuracy of the GPS data from any source.

Let's see what Bikeometers are actually on the Arlington Loop. I can use that data to pick some popular trails to visualize.

```{r}
sql_cmd <- "SELECT * FROM counts.bikeometer_details"
# creates a lazy table
all_bikeometer_table <- dbGetQuery(con, sql_cmd)
all_bikeometer_table$longitude <- as.numeric(all_bikeometer_table$longitude)
all_bikeometer_table$latitude <- as.numeric(all_bikeometer_table$latitude)
all_bikeometer_table
```

To figure out labeling, I used [this site](https://rafalab.github.io/dsbook/ggplot2.html).

```{r}
ggmap(arlington) + # creates the map "background"
  geom_point(data = bikeometer_table, 
             aes(x = longitude, y = latitude, color = region), 
             alpha = 0.5, 
             size = 10) +
              arlington_loop_layer +
  geom_text(data = bikeometer_table, nudge_x = 0.01, aes(x = longitude, y = latitude, color = region, 
                                         label = bikeometer_id, inherit.aes= FALSE))+
  theme_map()
```



For some reason, bikeometer_id 6 isn't in my database. I'll quickly covert my list of Bikeometers into integers so I can use the sorted() function in python to order them.

```{r include=FALSE, cache=TRUE}
library(reticulate)
```

```{python}
bikeometer_list = ['33','30','43','24','59','56','47','48','10','20',
                           '35','57','18','3','58','61','62','38','44','14',
                           '60','5','6','42','37','27','26','8','7','51','52',
                           '45','22','21','36','34','41','9','39','16','15',
                           '54','55','31','28','11','2','25','19']

# Convert to integers
bikeometer_list = [int(i) for i in bikeometer_list]
sorted_bikeometer_list = sorted(bikeometer_list)
sorted_bikeometer_list
```

With them ordered, I'll check to see which Bikeometers are in my database.

It looks like Bikeometers 6 and 52 are not in my database. When I request data from them for a few different time ranges, it looks like there is no data for either of them, which explains why they aren't in my database. One mystery solved!
