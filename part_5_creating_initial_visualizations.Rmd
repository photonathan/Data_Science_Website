---
title: "Visualizing Arlington Bikometers"
subtitle: "Part 5: Creating the Initial Visualizations"
output:
  html_document: 
    toc: yes
    toc_depth: 2
    toc_float: yes
    highlight: zenburn
    code_download: true
    df_print: paged
    includes:
      in_header: header.html
---
\ 
\ 

# Goal

To match the plot of the MnDOT, I'll use an area plot, so I use the same code but instead of geom_point(), I'll use geom_area(). I'll also need to plot two different sets of data on one plot: '2017-2019 Daily Average' and '2020 Daily Total'. To do this, I'll create a new view in SQL that calculates 2017-2019 daily averages in one column and includes the 2020 counts in a second new column. However, before I do that, the MnDOT report combined the 'In' and 'Out' directions for each day, so I'll start by doing that.

# Connect to MySQL Database

```{r echo = FALSE, message = FALSE, warning= FALSE}
library(ggplot2)
library(dplyr)
library(odbc)
library(reshape2)
library(tidyr)
con <- dbConnect(odbc::odbc(), .connection_string = "Driver={MySQL ODBC 8.0 Unicode Driver};", 
                 server = "localhost", db = "counts", user = "root", password = rstudioapi::askForPassword("Database password"))
```

# Combining 'In' and 'Out' Values

Right now, the table looks like this:

```{r}
counts_daily <- tbl(con, 'counts_daily')
head(counts_daily)
```

In MySQL, I'll create a new view in MySQL that takes the sum of both the 'I' and 'O' values and group them by 'date' and 'bikeometer_id'.

```{sql eval=FALSE}
CREATE OR REPLACE VIEW counts_daily_total AS
SELECT 
cd.bikeometer_id AS bikeometer_id,
cd.date AS date,
SUM(cd.count) AS count,
cd.is_weekend AS is_weekend
FROM counts_daily cd 
GROUP BY cd.date, cd.bikeometer_id;
```

For those not familiar with MySQL views, they look like tables but are actually recalculated each them they are accessed. This is what I want, because my python program imports the new Bike Arlington data into my 'counts_daily' MySQL table. Each time the 'counts_daily_total' view is accessed, it will automatically update to account for new data added to the 'counts_daily' table. Thus, I don't need to change my python program to generate new and useful tables in MySQL.

Now, let's take a look at our new view.

```{r}
counts_daily_total <- tbl(con, 'counts_daily_total')
head(counts_daily_total)
```

As you can see, the 'I' and 'O' directions are combined!

# Filtering for Date Range and Bikeometer ID

In a previous post, I chose a time-frame and which Bikeometers were best to plot. So I'll filter by creating a new view in MySQL called 'vw_filtered_counts_daily_total_2017_to_2019'.

```{sql eval=FALSE}
CREATE OR REPLACE VIEW vw_filtered_counts_daily_total_2017_to_2019 AS 
SELECT * FROM counts.counts_daily_total 
WHERE bikeometer_id IN (14,15,16,18,22,31,39) 
AND Year(Date) in (2017,2018,2019);
```

```{r paged.print=TRUE}
filtered_counts_daily_total_2017_to_2019 <- tbl(con, 'filtered_counts_daily_total_2017_to_2019')
filtered_counts_daily_total_2017_to_2019
```

# Creating the 2017-2019 Daily Average column

```{sql eval=FALSE}
CREATE OR REPLACE VIEW counts_daily_average_2017_to_2019
AS select 
cd.month_day as month_day,
AVG(cd.count) as average
FROM filtered_counts_daily_total cd
LEFT JOIN filtered_counts_daily_total a ON cd.month_day = a.month_day
group by cd.month_day;
```

```{r}
counts_daily_average_2017_to_2019 <- tbl(con, 'counts_daily_average_2017_to_2019')
counts_daily_average_2017_to_2019
```

Can use this as a select statement to view these two tables, minus the first line

```{sql eval=FALSE}
CREATE TABLE the_best_table AS
SELECT a.month_day, a.average_2017_to_2019, average_2020
FROM counts_daily_average_2017_to_2019 A 
INNER JOIN counts_daily_average_2020 B ON B.month_day = A.month_day
```

```{r message=FALSE}
df <- tbl(con, 'the_best_table')
# These Bikeometers are all located in the Arlington region
```

In order for this table to be easily read by R, we need to covert it from a wide table to a long table. [This]('https://mgimond.github.io/ES218/Week03b.html') is the resource I used to understand how to do the conversion.

```{r}
df.long <- pivot_longer(df, cols=2:3, names_to='year', values_to='counts')
```

Here I added the year to the end of the all the month_day values then convert that to a date class so R has an easy time graphing it.

```{r}
df.long <- mutate(df.long, month_day=paste(month_day, '-2020', sep = ''))
df.long <- collect(df.long)
```

Next, I converted each character string in the month_day column into a date class.

\#\`\`\`{r} df.long$month_day \<- mutate(month_day = as.Date(month_day, format='%m/%d/%Y'))

df.long

\#\`\`\`

Then I'll filter for the dates in my range

```{r}
df.long.filtered <- df.long %>% filter(month_day >= '03-12-2020' & month_day <= '05-15-2020') %>% collect()
df.long.filtered
```

Finally I convert the month_day values from a 'character class' to a time class. Be sure to lowercase the 'm' and 'd' in '%m-%d-%Y'.

```{r}
df.long.filtered$month_day <- strptime(df.long.filtered$month_day, '%m-%d-%Y')
class(df.long.filtered$month_day[1])
df.long.filtered$month_day <- as.Date(df.long.filtered$month_day)
class(df.long.filtered$month_day[1])
```

```{r}
#library(ggpmisc)
p <- df.long.filtered %>% ggplot(aes(x = month_day, y = counts)) +
  geom_line(aes(group = year, color = year))
  #scale_x_date()
  #stat_peaks(span = 30)
print(p)
```


```{r}
p <- df.long.filtered %>% ggplot(aes(x = month_day, y = counts)) +
  geom_area(aes(group = year, color = year))
print(p)
```

It looks like there definitely was an increase but it doesn't look as nice as the MnDOT plot. Let's investigate this a little.

# Bikeometer Locations

I'll map the locations of the Bikeometers to see where exactly they are in Arlington. To accomplish this, I found [this video](https://www.youtube.com/watch?v=2k8O-Y_uiRU&t=3s) by Professor Lisa Lendway very helpful along with [her website](https://mapping-in-r.netlify.app/).

![](images/Arlington%20Open%20Street%20Map.png "Arlington Open Street Map bbox")

On the left are the numbers we need to enter into the bbox variable below in order to get the graph from [Stamen Maps](http://maps.stamen.com/). I've chosen 'terrain' as my map type and the 'zoom' will be 13, which is in the openstreetmap url just after the text 'map='.

```{r message=FALSE}
library(ggmap)
library(ggthemes)
# Get the map information
arlington <- get_stamenmap(
    bbox = c(left = -77.2377, bottom = 38.8075, right = -76.9744, top = 38.9080), 
    maptype = "terrain",
    zoom = 12)
```

Next we will import the Bikeometer data from my database that contains the longitude and latitudes needed to plot the Bikeometers.

```{r}
sql_cmd <- "SELECT * FROM counts.bikeometer_details WHERE bikeometer_id in (14,15,16,18,22,31,39)"
# creates a lazy table
bikeometer_table <- dbGetQuery(con, sql_cmd)
bikeometer_table
```

One Bikeometer stands out as not even being in Alrington... how did that get in there? Let's plot it anyway to see how close it is to Arlington. Realistically, I frequently bike into Alexandria on the weekends but I'll probably end up excluding it none the less.

```{r}
# Convert latitude and longitude to integers
bikeometer_table$longitude <- as.numeric(bikeometer_table$longitude)
bikeometer_table$latitude <- as.numeric(bikeometer_table$latitude)

# Plot the points on the map
ggmap(arlington) + # creates the map "background"
  geom_point(data = bikeometer_table, 
             aes(x = longitude, y = latitude, color = region), 
             alpha = 0.5, 
             size = 3) +
             theme_map()
```

From my experience, the most popular bike trail in Arlington will be on the Arlington Loop. I want to get a sense if these Bikeometers are on the Arlington Loop, and if not, choose Bikeometers that are on the loop. My hypothesis is that bike trails that weren't popular before the pandemic won't see much change after the pandemic. I generally either ride the Arlington Loop or the W&OD Trails as they are the most bike-friendly. A trail that is less bike-friendly before the pandemic won't become that much more bike-friendly during a pandemic, even if there is a reduction of traffic in the city.

I'll use the GPS coordinates from my Garmin GPS watch to create a layer for my plot that shows the Arlington Loop trail. I downloaded the gpx file from the Garmin Connect website.

![](images/GPX%20file%20from%20Garmin%20Connect%20Website.jpg "Garmin Connect GPX")

I used the websites [GPXStudio](https://gpxstudio.github.io/) and [MyGeodata Converter](https://mygeodata.cloud/converter/gpx-to-csv) to remove unnecessary data points and convert from GPX to CSV respectively.

I read in the CSV which I'll use to plot the Arlington Loop.

```{r}
arlington_loop_table <- read.csv('~/Github/Arlington_Bikeometer_Visualizations/Data/arlington_loop.csv')
arlington_loop_table
```

I passed the 'arlington' map object to ggmap to set the background image to Arlington then plotted the Arlington Loop using geom_point. I used the 'longitude' column as the x-axis, 'latitude' column as the y-axis, 'alpha' is the transparency of the points (0 to 1) and 'size' is the size of each point. I used 'theme_map' without 'x' and 'y' axes.

```{r}
ggmap(arlington) + # creates the map "background"
  geom_point(data = arlington_loop_table, 
             aes(x = longitude, y = latitude), 
             alpha = 0.5, 
             size = .5) +
             theme_map()
```

Next, I'll create an Arlington Loop layer so I can overlay it on top of the Bikeometer location map. I found two options to create this layer: geom_polygon or geom_path. The biggest diffrence I see is that you can specify a fill for geom_polygon, but considering I don't want to fill the layer in, I'll use geom_path.

```{r}
ggmap(arlington) + geom_polygon(aes(x = longitude, y = latitude),
                             data = arlington_loop_table, fill = 'blue', alpha = 0.1, size = 1, color = "red")
```

```{r}
ggmap(arlington) + geom_path(data = arlington_loop_table, aes(x = longitude, y = latitude), size = 1, color = 'red') + theme_map()

```

If I wanted the path to change color according to elevation, I could use the below \<color = ele\> argument. This will generate a legend on the map. By adding the \<theme(legend.background = element_blank())\> code, you remove the background from the legend. However I won't be using this code.

```{r}
ggmap(arlington) + geom_path(data = arlington_loop_table, aes(x = longitude, y = latitude, color = ele), size = 1) + theme_map() + theme(legend.background = element_blank())
```

I'll assign the geom_path to the arlington_loop_layer variable to be plotted on top of the plot with the location of the Bikeometes.

```{r}
arlington_loop_layer <- geom_path(data = arlington_loop_table, aes(x = longitude, y = latitude), size = 1, color = 'red')
```

```{r}
ggmap(arlington) + # creates the map "background"
  geom_point(data = bikeometer_table, 
             aes(x = longitude, y = latitude, color = region), 
             alpha = 0.5, 
             size = 3) +
              arlington_loop_layer +
             theme_map()
```

It looks like 1 or 2 Bikeometers are on the trail, I'm not sure of the accuracy of the GPS data from any source.

Let's see what Bikeometers are actually on the Arlington Loop. I can use that data to pick some popular trails to visualize.

all_bikeometer_table

```{r}
sql_cmd <- "SELECT * FROM counts.bikeometer_details"
# creates a lazy table
all_bikeometer_table <- dbGetQuery(con, sql_cmd)
all_bikeometer_table$longitude <- as.numeric(all_bikeometer_table$longitude)
all_bikeometer_table$latitude <- as.numeric(all_bikeometer_table$latitude)
all_bikeometer_table
```

To figure out labeling, I used [this site](https://rafalab.github.io/dsbook/ggplot2.html).

```{r}
ggmap(arlington) + # creates the map "background"
  geom_point(data = bikeometer_table, 
             aes(x = longitude, y = latitude, color = region), 
             alpha = 0.5, 
             size = 3) +
              arlington_loop_layer +
  geom_text(data = bikeometer_table, aes(x = longitude, y = latitude, color = region, label = bikeometer_id))+
  theme_map()
```

For some reason, bikeometer_id 6 isn't in my database. I'll quickly covert my list of Bikeometers into integers so I can use the sorted() function in python to order them.

```{r include=FALSE}
library(reticulate)
```

```{python}
bikeometer_list = ['33','30','43','24','59','56','47','48','10','20',
                           '35','57','18','3','58','61','62','38','44','14',
                           '60','5','6','42','37','27','26','8','7','51','52',
                           '45','22','21','36','34','41','9','39','16','15',
                           '54','55','31','28','11','2','25','19']

# Convert to integers
bikeometer_list = [int(i) for i in bikeometer_list]
sorted_bikeometer_list = sorted(bikeometer_list)
sorted_bikeometer_list
```

With them ordered, I'll check to see which Bikeometers are in my database.

It looks like Bikeometers 6 and 52 are not in my database. When I request data from them for a few different time ranges, it looks like there is no data for either of them, which explains why they aren't in my database. 
