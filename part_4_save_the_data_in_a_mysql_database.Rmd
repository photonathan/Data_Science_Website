---
title: "Visualizing Arlington Bikometers"
subtitle: "Part 4: Save your Data in a MySQL Database"
output:
  html_document: 
    toc: yes
    toc_depth: 2
    toc_float: yes
    highlight: zenburn
    code_download: true
    includes:
      in_header: header.html
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache=TRUE)
```

# Background
I will be building on the Python code we wrote in [Part 2: Query the API](https://nathansprojects.com/part_2_query_the_api.html) to move our data into a database. 

# Goals
1. Add functionality to our functions that takes a Pandas Data Frame and puts it in our MySQL database. 
2. Create a few  **helper functions** that our main functions will call.
3. Finalize our functions to pull the API data and upload it to the MySQL database.

## Steps
1. Moving Bikeometer Details to database
2. Moving Daily Bike Counts to database
3. Add feature to check for existing data in the database.
4. Add feature that only pulls and adds new data to the database.
5. 

# Step 1: Move our data into MySQL

## Migrating Bikeometer Details

We will utilize the Pandas module to move our data into MySQL with the [to_sql](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_sql.html) method. This method will operate on our dataframe and we will pass in a few parameters. First is our table name that we will create. Second is our engine that was created using SQLAlchemy. Third we specify to not create a column as the index. Last, we specify that if our table already exists in the database, add our data to the table without touching the existing data.

```{python eval=FALSE}
df.to_sql('counts_daily', con=engine, index=False, if_exists='append')
```

Let's work on adding the **to_sql** method to our code that pulls the Bikeometer details.

First, let's define a **helper function** that will create a SQLAlchemy *engine* object. Let's call the function 'create_new_engine'. All we have to do is pass in the name of the database as an argument (bikeometers_db).

```{python eval=FALSE}
def create_new_engine(db_name):
    return create_engine(f'mysql+mysqldb://{input("Enter username: ")}:{getpass("Enter password: ")}@localhost/{db_name}', echo=False)
```

```{python include=FALSE}
# Workaround to get this document to Knit
# Created temp user name that will be deleted once the post is uploaded.
def create_new_engine(db_name):
  return create_engine(f'mysql+mysqldb://newuser:donteventry@localhost/bikeometers_db', echo=False)
```

I've taken the *get_bikeometer_details* function from Part 2 as well as the GET request steps and made them into a single function *bikeometer_to_sql*.

```{python}
import requests
import re
import xml.etree.ElementTree as ET
import pandas as pd
from sqlalchemy import create_engine

def bikeometer_to_sql():
    '''
    Makes a GET request to the Bike Arlington API using the GetAllCounters 
    method as a parameter. The API returns a response object that is first
    converted to a string, cleaned, and converted to an XML object. The XML 
    object is then parsed for the relevant information, and added to a list.
    Each list, representing a single Bikeometer, is converted to a tuple and added 
    to a final list which is converted to a dataframe and uploaded to the database. 
     '''
    # Calls our helper function to create an engine.
    engine = create_new_engine('bikeometers_db')
    url = 'http://webservices.commuterpage.com/counters.cfc?wsdl'
    # Defines the method in a dictionary used to make the request
    counter_reqest_methods = {'method': 'GetAllCounters'}
    # Assigns response object to variable 'response'
    response = requests.get(url, params=counter_reqest_methods)
    # Save the content of that request (string) to memory 
    string_data = response.text
    # Clean the string
    clean_string_data = re.sub(r'[\n|\t]', '', string_data)
    # Convert string to XML object
    root = ET.fromstring(clean_string_data)
    # Create the empty list that will include [(Name, counterID, Lat, Long, Region, region_id)(...)]
    bikeometer_details = []
    # Iterate through the children, grandchildren, and great-grandchildren and grab the data  
    for child in root:
        # From child 'counter' gets the attribute 'id' of the counter and adds it to single_list
        single_list = [child.get('id')]
        # Loops through the grandchildren of root
        for grandchild in list(child):   
            if grandchild.text != None and grandchild.tag != 'description' and grandchild.tag != 'trail_id' and grandchild.tag != 'trail_name':
                # If the grandchild is region, loop through region and grab the grandchildren data
                if grandchild.tag == 'region':  
                    for great_grandchild in list(grandchild): 
                        single_list.append(great_grandchild.text)
                # If the grandchild is not region, the data is available in grandchild.text
                else: 
                    single_list.append(grandchild.text)
        # Cast the list into a tuple making it easier to migrate data to the database
        single_tuple = tuple(single_list)
        # Appends tuples to the list
        bikeometer_details.append(single_tuple)
    # Establishes the names of the columns
    columns = ('bikeometer_id', 'name', 'latitude', 'longitude', 'region', 'region_id')
    # Creates a pandas data frame
    df = pd.DataFrame(data=bikeometer_details, columns = columns)
    # Assigns 'types' to each column
    df[["name", "latitude", "longitude", "region", 'region_id']] = df[["name", "latitude", "longitude", "region", 'region_id']].astype('str')
    df[["bikeometer_id"]] = df[["bikeometer_id"]].astype('int')
    # Moves data to MySQL
    df.to_sql('bikeometer_details', con=engine, index=False, if_exists='replace')

```

Now we run the function.
```{python}
bikeometer_to_sql()
```
Let's check to see what our new table looks like. 

First we create a connection and assign it to *con*.
```{python}
con = engine.connect()
```
Then, we execute a MySQL statement.
```{sql connection=con}
SELECT * FROM bikeometer_details
```

Now that we have our bikeometer_details table in our database, we won't need to run this code again unless a new Bikeometer is added.

Next, we will request the daily bike counts for every day since a counter was installed and save it in our database. 

## Migrating Bike Counts

Again, we will build on the code from Part 2.

In Part 2, we created a function called **api_counts_to_list** which pulled the bicycle counts in a hard-coded start date, end date, and a single Bikeometer. However, now we need to pull the data for **all** Bikeometers.

To make our life easier, let's select the oldest Bikeometer, pull it's first date on the job, and use that date as our *start date* for all Bikeometers. Our function is writted so that if there is no data for a requested date, our function just skips those dates and moves on without error.

We will call the built-in Bike Arlington API method: 'getMinDates' on all the Bikeometer IDs in the below *bikeometer_id_list*.

```{python}
bikeometer_id_list = ['33','30','43','24','59','56','47','48','10','20',
                           '35','57','18','3','58','61','62','38','44','14',
                           '60','5','6','42','37','27','26','8','7','51','52',
                           '45','22','21','36','34','41','9','39','16','15',
                           '54','55','31','28','11','2','25','19']
```

Now, we will create a simple *for loop* to call the *getMinDates* method on all the Bikeometers.
```{python}
# Assign the url of the 
url = 'http://webservices.commuterpage.com/counters.cfc?wsdl'

# Defines the method in a dictionary used to make the request
for bikeometer_id in bikeometer_id_list: 
  counter_reqest_methods = {'method': 'getMinDates', 'counterID': bikeometer_id}
  # Save the GetAllCounters request to memory
  response = requests.get(url, params=counter_reqest_methods)
  response.text

```
Skimming through the returned responses, we can see that the first date was 4/1/2010. This will be our first *start date* in our function. 

Ultimately, we will have two functions when we are finished: one that pulls all the dates starting from 4/1/2010 and one that pulls only the new dates. Let's create the first function called **all_counts_by_date_to_sql**.


```{python}
from datetime import date, datetime, timedelta

def api_counts_to_list(): 
  for bikeometer_id in bikeometer_id_list:
    request_parameters = {'method': 'GetCountInDateRange',
                      'counterID': bikeometer_id,
                      'startDate': '07/22/2015' ,
                      'endDate': '07/22/2015',
                      'mode': 'B',
                      'interval': 'D',
                      'direction': ''}
    response = requests.get(url, params=request_parameters)
    string_data = response.text
    clean_string_data = re.sub(r'[\n|\t]', '', string_data)
    root = ET.fromstring(clean_string_data)
    for type_tag in root.findall('count'):
        count = type_tag.get('count')
        date = type_tag.get('date')
        # Converts counter date to a date object
        date = datetime.strptime(date, '%m/%d/%Y').date()
        year = date.year
        month = date.month
        day = date.day
        month_day = f'{month}_{day}'
        direction = type_tag.get('direction')
        if date.weekday() <= 4:
            is_weekend = 0
        else:
            is_weekend = 1
        single_tuple = (bikeometer_id, date, direction, count, is_weekend, year, month, day, month_day)
        count_in_date_range_list.append(single_tuple)
  return count_in_date_range_list
all_id_list = api_counts_to_list()
all_id_list
```

```{python}
# Assign the url of the 
url = 'http://webservices.commuterpage.com/counters.cfc?wsdl'
# Defines the method in a dictionary used to make the request
counter_reqest_methods = {'method': 'GetCountInDateRange'}
# Save the GetAllCounters request to memory
response = requests.get(url, params=counter_reqest_methods)
response
```

Perfect, response [200]! Let's take a look inside our 'response object' by using the **.text**

```{python}
response.text
```

It looks like we're missing some parameters... which makes sense. If we take a look at the documenation, it says: 

"Request Fields:
• CounterID – Number Required Field
• startDate – mm/dd/yyyy
• endDate – mm/dd/yyyy
• direction – I, O (I: Inbound, O: outbound), Empty for both.
• mode – B, P (B: bike, P: pesdestrian), empty for both.
• startTime – HH:MM format
• endtime – HH:MM format
• interval – h (by the hourly), m(by the minutes), d(by the day

Example with interval as d:
http://webservices.commuterpage.com/counters.cfc?wsdl&method=GetCountInDateRan
ge&counterid=1&startDate=12/1/2011&endDate=12/04/2011&direction=I&mode=B&inte
rval=d"

Like with many parts of coding, there are multiple ways to achieve the same goal. We could create a function that concatenates a URL according to our desired 'request fields', or we can do what I do below: pass in each of the request fields to the GET request.

I'll start with the first 'bikeometer_id' in the above Bikeometer Details data frame, so counterID = 33. To get the 'start date', let's use one of the built-in Bike Arlington API methods: 'getMinDates'

```{python}
# Assign the url of the 
url = 'http://webservices.commuterpage.com/counters.cfc?wsdl'
# Defines the method in a dictionary used to make the request
counter_reqest_methods = {'method': 'getMinDates', 'counterID': '33'}
# Save the GetAllCounters request to memory
response = requests.get(url, params=counter_reqest_methods)
response.text
```
The first date for Bikeometer 33 is 07/22/2015.

Just to test, let's make the end date the same as the start date, 07/22/2015. For 'mode', we have the option to request data for Bikers, Pedestrians, or Both. I'll choose Bikers, so the mode will be 'B'. For 'interval' we can choose 'minute', 'hourly', or 'daily'. I'll choose 'daily'. I'll leave 'direction' blank to pull both the Inbound and Outbound direction.

We will organize all these parameters in a dictionary and pass that dictionary to the GET method.

```{python}
request_parameters = {'method': 'GetCountInDateRange',
                      'counterID': '33',
                      'startDate': '07/22/2015' ,
                      'endDate': '07/22/2015',
                      'mode': 'B',
                      'interval': 'D',
                      'direction': ''}
response = requests.get(url, params=request_parameters)
response.text
```
It looks like on 7/22/15, there were 384 bikers in the Inbound direction and 399 bikers in the Outbound direction. Before we extract this data, let's clean it up a little just to make our lives easier.

## Step 2: Clean the Data

Again, we will use the 're' module to remove some of the extraneous html. 

The code below will substitute any '\n' or '\t' with a blank string (''), essentially deleting them.

```{python}
import re

string_data = response.text
clean_string_data = re.sub(r'[\n|\t]', '', string_data)
clean_string_data
```

Again, The first part of the data mentions that the data is in XML format. We can use this to our advantage by converting the string to an XML to easily pull out the data we are looking for.

## Step 3: Convert data to XML

This step is the same as above but I'll repeat it again for those that skipped right to this step. 

We will use the module 'xml.etree.ElementTree' to convert the data from a string to XML format. By converting this to XML, finding the data we need will be faster because we don't have to iterate over the entire string looking for the specific data we need, we can use the hierarchical structure of an XML file to drill down to the specific data we are looking for.

To make it easier to call, we will import the module as 'ET'. We will then call the method 'fromstring' and pass in our 'clean_string_data' as an argument.

```{python}
import xml.etree.ElementTree as ET

root = ET.fromstring(clean_string_data)
type(root)
```

Our object 'root' is now an 'xml.etree.ElementTree.Element' object that we can iterate over with a **for loop** or we can use the **list** function.

## Step 4: Explore XML Data

Using the **list** function allows us to take a look inside **root** to see its children.

```{python}
list(root)
```

We can see there are two "Element 'counter'..." objects, one representing the Inbound direction and the other is the Outbound direction on our requested date.

Let's slice into the list of counters and assign the first counter to the variable 'child' then take a look inside.

```{python}
child = root[0]
child.items()
```

We can see that 'child' contains the 'count', 'date', 'direction', and 'mode'.

To isolate the important information we can use the **get** method and pass in 'count', 'date', 'direction', or 'mode' to get the value.

```{python}
child.get('count')
child.get('date')
child.get('direction')
child.get('mode')
```
Now that we know how to pull the data that we want, let's automate it.

## Step 5: Automate With A Function

I'll show you the function that I created to iterate through the XML data. Every 'count' and its details will be saved in a tuple and those tuples will be saved all together in a list.

First, I'll create the empty list that will house the 'count' tuples.

```{python}
count_in_date_range_list = []
```

Next, I'll create a list of the Bikeometer ID's that I'd like to pull data for. My function will iterate over this list using a **for loop**.

```{python}
bikeometer_id_list = ['33','30','43','24','59','56','47','48','10','20',
                           '35','57','18','3','58','61','62','38','44','14',
                           '60','5','6','42','37','27','26','8','7','51','52',
                           '45','22','21','36','34','41','9','39','16','15',
                           '54','55','31','28','11','2','25','19']
```


Next, I'll create a Russian nesting doll of a **for loop** to dive into the appropriate sections to pull the information I want. I'll use the **datetime** module to separate out the 

```{python}
from datetime import date, datetime, timedelta

def api_counts_to_list(): 
  for bikeometer_id in bikeometer_id_list:
    request_parameters = {'method': 'GetCountInDateRange',
                      'counterID': bikeometer_id,
                      'startDate': '07/22/2015' ,
                      'endDate': '07/22/2015',
                      'mode': 'B',
                      'interval': 'D',
                      'direction': ''}
    response = requests.get(url, params=request_parameters)
    string_data = response.text
    clean_string_data = re.sub(r'[\n|\t]', '', string_data)
    root = ET.fromstring(clean_string_data)
    for type_tag in root.findall('count'):
        count = type_tag.get('count')
        date = type_tag.get('date')
        # Converts counter date to a date object
        date = datetime.strptime(date, '%m/%d/%Y').date()
        year = date.year
        month = date.month
        day = date.day
        month_day = f'{month}_{day}'
        direction = type_tag.get('direction')
        if date.weekday() <= 4:
            is_weekend = 0
        else:
            is_weekend = 1
        single_tuple = (bikeometer_id, date, direction, count, is_weekend, year, month, day, month_day)
        count_in_date_range_list.append(single_tuple)
  return count_in_date_range_list
all_id_list = api_counts_to_list()
all_id_list
```
Each tuple contains: (bikeometer_id, date, direction, count, is_weekend, year, month, day, month_day).

I added a few more variables by manipulating the date a bit in order to make the analysis later easier.

## Step 6: Data into a data frame

First, we will load that **Pandas** module so we can turn our list of tuples into a data frame.

```{python}
import pandas as pd
```

Next, we will define the columns of the data frame according to the data in our tuples. 
```{python}
columns = ('bikeometer_id', 'date', 'direction', 'count', 'is_weekend', 'year', 'month', 'day', 'month_day')
```

With our columns defined, we will create a dataframe using the pandas **DataFrame** method, pass in our **bikeometer_details** list into the **data** parameter and pass in our **columns** variable to the **columns** parameter.
```{python}
df = pd.DataFrame(all_id_list, columns=columns)
df
```

Now that we are able to automate pulling the data into a data frame, in the next post, we can set up our database which we will use to store the data for analysis. 