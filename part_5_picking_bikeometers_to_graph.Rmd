---
title: "Visualizing Arlington Bikometers"
subtitle: "Part 5: Picking Which Bikeometers to Graph"
output:
  html_document: 
    toc: yes
    toc_depth: 2
    toc_float: yes
    highlight: zenburn
    code_download: true
    includes:
      in_header: header.html
---

# Goal

Determine which Bikeometers to include in our visualization. Bikeometers will be excluded if they are missing data in the time range we are exploring. Bikeometers were also excluded if they were not in the Arlington region.

# Import the table

We will start by importing the necessary packages and establishing a connection to the database.

```{r echo = FALSE, message = FALSE, warning= FALSE}
library(ggplot2)
library(dplyr)
library(odbc)

con <- dbConnect(odbc::odbc(), .connection_string = "Driver={MySQL ODBC 8.0 Unicode Driver};", 
                 server = "localhost", db = "bikeometers_db", user = "root", 
                 password = rstudioapi::askForPassword("Database password"))
```

Next, we can use the dplyr function tbl(), pass in the connection object *con* and the name of the table we want to look at 'counts_daily'. This will create a 'lazy table', importing only the first 10 rows. You can tell the entire table has not been imported because instead of listing the number of rows, it lists: 'Source: table<counts_daily> [?? x 9]'

```{r paged.print=TRUE}
# Imports to a lazy table
counts_daily <- tbl(con, 'counts_daily')
counts_daily
```

The MnDOT combines directions 'I' and 'O' together so let's do that now.

# Combine I and O values

In order to create our new 'combined' table, we want 'date = 2010-12-08, bikeometer_id=3, direction=I' combined with 'date = 2010-12-08, bikeometer_id=3, direction=O'. In order to do this, we will need to group our observations not only by date, but also by bikeometer_id. Remember, there are many observations with date=2010-12-08 as each Bikeometer may have data for that date.

We will pipe our counts_daily table into the group_by() and summarize() functions found in the *dplyr* package. To combine the 'I' and 'O' counts we will use the function sum() then just specify the other variables we want to include in our new *counts_daily_total* table.

```{r}
counts_daily_total <- counts_daily %>% 
  group_by(date, bikeometer_id) %>% 
  summarize(count = sum(count), is_weekend = is_weekend, 
            month = month, day = day, year = year, month_day = month_day) %>% 
  collect()
counts_daily_total

```

To do a quick check, we can add the 'I' and 'O' values for bikeometer_id=3, date=2010-12-08 (16+96=112). Looking at our new table, we can look at the first observation to see that it worked.

Even though it isn't mentioned in the Bike Arlington documentation, it's important to account for the 'A' direction. When the Bikeometer can't distinguish between outbound and inbound directions, the direction is classified as 'A'.

Let's use Bikeometer 14 as an example and check to make sure our 'A' *counts* have been properly moved over to our table.

```{r}
counts_daily_a <- counts_daily_total %>% filter(bikeometer_id == '14') %>% collect()
counts_daily_a
```

Next, we will confirm that these counts match the counts in the original *counts_daily* table.

```{r}
counts_daily_a <- counts_daily %>% filter(bikeometer_id == '14') %>% collect()
counts_daily_a
```

The only difference is that we lost the *direction* variable in our *counts_daily_total* table which we don't need anymore.

# Creating the inital visualizations

This is where our hard work will start to pay off. Let's use the date's we chose in Part 1 (March 12 - May 15), pick a Bikeometer (28), and create a graph.

First, we will filter our data. We will pipe our *counts_daily_total* table into the *dplyr:filter* function to subset rows using column values.

```{r}
counts_daily_filtered <- counts_daily_total %>% 
  filter(date >= '2017-03-12' & date <= '2017-05-15' & bikeometer_id == '28'| 
         date >= '2018-03-12' & date <= '2018-05-15' & bikeometer_id == '28'|
         date >= '2019-03-12' & date <= '2019-05-15' & bikeometer_id == '28'| 
         date >= '2020-03-12' & date <= '2020-05-15' & bikeometer_id == '28') %>%
  print()
```

Another way to do this is to use the **dbGetQuery()** function in the *DBI* package. We can pass in the MySQL SELECT statement directly to the dbGetQuery() function along with the connection object to return our desired table. 

Using head(my_table), I was able to view the top 6 rows of my_filtered_table directly in R.

```{r}
sql_cmd <- "SELECT * FROM bikeometers_db.counts_daily WHERE bikeometer_id = 28 AND 
date >= '2020-03-12' AND date <= '2020-05-15' ORDER BY count desc"
# creates a lazy table
my_filtered_table <- dbGetQuery(con, sql_cmd)
# displays the top 6 rows
head(my_filtered_table)
# When you are done, run: dbDisconnect(con)
```


**Now** we can create a plot! We will pipe our counts_daily_filtered table into the *ggplot* package, define the aesthetics, and call the geom_point() function to make a scatterplot.

```{r}
counts_daily_filtered %>% ggplot(aes(date, count)) +
    geom_point()
```

Very cool! Let's see what the actual highest counts are by sorting the rows by descending 'counts'.

# Order by descending counts

We can use the dplyr function *arrange()*, specify the variable name (count) and specify to sort in descending order.

```{r}
counts_daily_filtered %>% arrange(desc(count))
```

It looks like on May 2, 2020 there were 2669 bikers recorded by Bikeometer 28.

We can also see that in 2020 there are some dates with 0 bikers recorded. As I've chosen to only include Bikometers without missing data, we can eliminate this Bikeometer.

# Function to Plot Every Bikeometer

To get a better view of the data, we will need to plot every Bikeometer individually.

From what I've [read](https://stackoverflow.com/questions/7142767/why-are-loops-slow-in-r), using 'for loops' are slow in R but I haven't been able to find another way to generate plots this easily.

So, we can use a nested *for loop* to easily generate plots for each counter in the Arlington region. I've commented out all but one Bikeometer ID (28) to only generate plots for one Bikeometer for this post, but to test out the code, you can un-comment the rest of the vector to generate all the plots yourself.

```{r message=FALSE, paged.print=TRUE}
for (id in c(33)){#28,30,24,59,10,20,18,3,58,61,62,14,60,5,6,27,26,8,7,22,21,9,16,15,31,11,2,25,19)){
  for (year in c(2017,2018,2019,2020,2021)){
    start_date <- paste(year, '-03-12', sep = '', collapse = '-')
    end_date <- paste(year, '-05-15', sep = '', collapse = '-')
    counts_daily_filtered <- counts_daily_total %>% filter(date >= start_date & date <= end_date & bikeometer_id == id) %>% collect()

    p <- counts_daily_filtered %>% ggplot(aes(date, count)) +
    geom_point() +
    labs(
      title = 'Daily Bicycle Counts',
      subtitle = paste('Year = ', year, 'ID = ', id)) +
    xlab('Date') +
    ylab('Counts Daily') +
    theme(
      plot.title = element_text(hjust = 0.5, vjust = 1),
      plot.subtitle = element_text(hjust = 0.5, vjust = -1, size = 10, face = 'italic')
      ) 
    print(p)
    #ggsave(filename = paste('ID ', id, ' Year ', year,' counts', '.jpg', sep = ''), path = '~/Github/Arlington_Bikeometer_Visualizations/Output', device = 'jpg', dpi = 'retina')
}}
```

Yes! The plots were generated perfectly. However, our joy is short-lived as we notice big holes in 2019 and 2020 plots.

# Missing data or my mistake?

We can see that in 2019 and 2020 there is some missing data. In 2020, there is over a week of 0 counts and in 2019 the counts are just missing. 

When I manually queried the [Bike Arlingon web interface](%22http://counters.bikearlington.com/%22) for the 2020 dates, there is data in their database with count=0.

[![2020 Web Query](images/id_33_2020_web_query.jpg "2020 Web Query")](http://webservices.commuterpage.com/counters.cfc?wsdl&method=GetCountInDateRange&counterid=33&startDate=03/12/2020&endDate=05/15/2020&direction=I&mode=B&interval=d)

However, for the 2019 missing dates, there was no data available.

[![2019 Web Query](images/id_33_2019_web_query.jpg "2019 Web Query")](http://webservices.commuterpage.com/counters.cfc?wsdl&method=GetCountInDateRange&counterid=33&startDate=03/12/2019&endDate=05/15/2019&direction=I&mode=B&interval=d)

If I were to recreate my database of counts, I would include a clause 'if there is no data to import for a time-point, count = 0'. The Bike Arlington people probably have a similar clause, which probably prevents the web interface from breaking when a vaild query is made but no data is in the server.

# Choosing the Bikeometers

After confirming that the missing data wasn't in the database and looking through the plots of each Bikeometer, I found these Bikeometers without missing data: 14,15,16,18,22,31,39.

# Filling in the missing data?

The MnDOT released their data quality management procedures in [Minnesota's Walking and Bicycling Data Collection Report Update](https://www.dot.state.mn.us/bike-ped-counting/reports/2018-2019%20MinnesotaPedBikeCountReport.pdf). Here, they list how they remove outlier, review suspicious data points, create a linear regression model, and use said regression model to **estimate daily traffic for missing days**. In the future, I'd like to preform a similar analysis to fill in the missing data.

The [next post](https://nathansprojects.com/part_6_creating_initial_visualizations.html) will be about creating the geom_area plot that matches the [MnDOT's plot](https://storymaps.arcgis.com/stories/70a3a57003b541ed8e911cd11103dc6a).